{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d792b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_datareader.data as web\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.regime_switching.markov_regression import MarkovRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from utils import *\n",
    "from models import *\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848bc0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_macro_data()\n",
    "returns = load_sp500_excess_returns()\n",
    "pca = compute_pca(df)\n",
    "df =merge_macro_excess_returns(df, returns)\n",
    "horizons = create_horizons(df)\n",
    "\n",
    "df = add_macro_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2945e08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de242ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, s_samples_post_yt = estimate_cay_MS_via_gibbs(df, verbose=True)\n",
    "df, s_samples_post_yt_at = estimate_cay_MS_logyt_logat(df, verbose=True)\n",
    "df, s_samples_post_pca = estimate_cay_MS_via_gibbs(df, model = 'pca', verbose=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e444f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, model_fc_yt = estimate_cay_FC_yt(df)\n",
    "df, model_fc_yt_at = estimate_cay_FC_yt_at(df)\n",
    "df, model_fc_pca = estimate_cay_FC_pca(df)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be039273",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_model_cay_yt = MarkovRegression(df['cay_FC_yt'], k_regimes=2, trend='c',\n",
    "                                switching_variance=True)\n",
    "ms_result_cay_yt = ms_model_cay_yt.fit(em_iter=0)\n",
    "ms_model_cay_yt_at = MarkovRegression(df['cay_FC_yt_at'], k_regimes=2, trend='c',\n",
    "                                switching_variance=True)\n",
    "ms_result_cay_yt_at = ms_model_cay_yt_at.fit(em_iter=0)\n",
    "ms_model_cay_pca = MarkovRegression(df['pca'], k_regimes=2, trend='c',\n",
    "                                switching_variance=True, )\n",
    "ms_result_cay_pca = ms_model_cay_pca.fit(em_iter=0)\n",
    "\n",
    "print(ms_result_cay_yt.summary())\n",
    "print(ms_result_cay_yt_at.summary())\n",
    "print(ms_result_cay_pca.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e5d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_controls = ['interest_rate', 'CPI_inflation', 'unemployment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5965adec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_model_cay_yt_with_macro = MarkovRegression(df['cay_FC_yt'], k_regimes=2, trend='c', exog=df[macro_controls],\n",
    "                                switching_variance=True)\n",
    "ms_result_cay_yt_with_macro = ms_model_cay_yt.fit(em_iter=0)\n",
    "ms_model_cay_yt_at_with_macro = MarkovRegression(df['cay_FC_yt_at'], k_regimes=2, trend='c', exog=df[macro_controls],\n",
    "                                switching_variance=True)\n",
    "ms_result_cay_yt_at_with_macro = ms_model_cay_yt_at.fit(em_iter=0)\n",
    "ms_model_cay_pca_with_macro = MarkovRegression(df['pca'], k_regimes=2, trend='c', exog=df[macro_controls],\n",
    "                                switching_variance=True, )\n",
    "ms_result_cay_pca_with_macro = ms_model_cay_pca.fit(em_iter=0)\n",
    "\n",
    "print(ms_result_cay_yt.summary())\n",
    "print(ms_result_cay_yt_at.summary())\n",
    "print(ms_result_cay_pca.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90d1fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4), sharex=True)\n",
    "\n",
    "smoothed_probs = ms_result_cay_yt.smoothed_marginal_probabilities\n",
    "smoothed_probs.plot(ax=axes[0], title='cay_yt regimes')\n",
    "axes[0].set_ylabel('Probability')\n",
    "axes[0].set_xlabel('Date')\n",
    "\n",
    "smoothed_probs = ms_result_cay_pca.smoothed_marginal_probabilities\n",
    "smoothed_probs.plot(ax=axes[1], title='cay_pca regimes')\n",
    "axes[1].set_ylabel('Probability')\n",
    "axes[1].set_xlabel('Date')\n",
    "\n",
    "smoothed_probs = ms_result_cay_yt_at.smoothed_marginal_probabilities\n",
    "smoothed_probs.plot(ax=axes[2], title='cay_yt_at regimes')\n",
    "axes[2].set_ylabel('Probability')\n",
    "axes[2].set_xlabel('Date')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617c176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_yt = run_multi_chain_gibbs(\n",
    "    df=df,\n",
    "    gibbs_function=estimate_cay_MS_via_gibbs_final,\n",
    "    num_chains=4,\n",
    "    num_iterations=10000,\n",
    "    burn_in=2000,\n",
    "    k_regimes=2,\n",
    "    model='yt',\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737926e9",
   "metadata": {},
   "source": [
    "1 Parameter Œ±‚ÇÄ (alpha_0)\n",
    "Left (density): Chains overlap better compared to before but still show multiple peaks (potential label switching or multimodality).\n",
    "\n",
    "Right (traceplot): Chains show better mixing than before but still some divergence.\n",
    "\n",
    "‚û°Ô∏è Interpretation: Improved convergence, but chains may still explore separate modes.\n",
    "\n",
    "üî¨ 2. Parameter Œ±‚ÇÅ (alpha_1)\n",
    "Left: Modes are closer, better overlap.\n",
    "\n",
    "Right: Chains are more stable but still show drift.\n",
    "\n",
    "‚û°Ô∏è Interpretation: Partial convergence, better than previous 2k runs.\n",
    "\n",
    "üî¨ 3. Parameter Œ≤‚ÇÄ (beta_0)\n",
    "Left: All chains collapse to a tight high peak at ‚âà1.\n",
    "\n",
    "Right: Traceplots are stable with little variation.\n",
    "\n",
    "‚û°Ô∏è Interpretation: Converged.\n",
    "\n",
    "üî¨ 4. Parameter Œ≤‚ÇÅ (beta_1)\n",
    "Left: Chains have overlapping densities but remain multimodal.\n",
    "\n",
    "Right: Traceplots drift and do not mix fully.\n",
    "\n",
    "‚û°Ô∏è Interpretation: Partial convergence, label switching possible.\n",
    "\n",
    "üî¨ 5. Parameter œÉ¬≤ (sigma2)\n",
    "Left: Chains have overlapping densities.\n",
    "\n",
    "Right: Traceplots stable, no drift.\n",
    "\n",
    "‚û°Ô∏è Interpretation: Good convergence.\n",
    "\n",
    "‚ö†Ô∏è Overall Project Convergence Status (10,000 Iterations)\n",
    "‚úÖ Improved convergence compared to 2k runs\n",
    "‚ö†Ô∏è Remaining issues:\n",
    "\n",
    "Label switching (especially in Œ±_s parameters)\n",
    "\n",
    "Possible model identification challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78f3042",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_pca = run_multi_chain_gibbs(\n",
    "    df=df,\n",
    "    gibbs_function=estimate_cay_MS_via_gibbs_final,\n",
    "    num_chains=4,\n",
    "    num_iterations=10000,\n",
    "    burn_in=2000,\n",
    "    k_regimes=2,\n",
    "    model='pca',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80625412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import invgamma, norm\n",
    "\n",
    "def estimate_cay_MS_gibbs_macro(df, k_regimes=2, n_iter=1000, burn_in=200, model='yt', verbose=True):\n",
    "    \"\"\"\n",
    "    Gibbs sampler for regime-switching model with macro controls as regime-invariant predictors.\n",
    "    \"\"\"\n",
    "    # ---------------------------\n",
    "    # 0. Data preparation\n",
    "    # ---------------------------\n",
    "    y = df['log_ct'].values\n",
    "    if model == 'yt':\n",
    "        X_main = df['log_yt'].values\n",
    "    elif model == 'pca':\n",
    "        X_main = df['pca'].values\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model\")\n",
    "\n",
    "    # Macro controls\n",
    "    Z = df[['interest_rate', 'CPI_inflation', 'unemployment']].values\n",
    "    T = len(y)\n",
    "    n_macro = Z.shape[1]\n",
    "\n",
    "    # ---------------------------\n",
    "    # 1. Hyperparameters\n",
    "    # ---------------------------\n",
    "    sigma2_y = np.var(y)\n",
    "    mu_alpha = np.zeros(k_regimes)\n",
    "    sigma2_alpha = np.ones(k_regimes) * sigma2_y\n",
    "    mu_beta = np.zeros(k_regimes)\n",
    "    sigma2_beta = np.ones(k_regimes) * sigma2_y\n",
    "\n",
    "    mu_gamma = np.zeros(n_macro)\n",
    "    sigma2_gamma = np.ones(n_macro) * sigma2_y\n",
    "\n",
    "    alpha_sigma = 2.5\n",
    "    beta_sigma = 0.5\n",
    "    prior_trans = np.ones((k_regimes, k_regimes)) * 0.5\n",
    "\n",
    "    # ---------------------------\n",
    "    # 2. Initialize storage\n",
    "    # ---------------------------\n",
    "    alpha_samples = np.zeros((n_iter, k_regimes))\n",
    "    beta_samples = np.zeros((n_iter, k_regimes))\n",
    "    gamma_samples = np.zeros((n_iter, n_macro))\n",
    "    sigma2_samples = np.zeros(n_iter)\n",
    "    s_samples = np.zeros((n_iter, T))\n",
    "\n",
    "    # Initialize parameters\n",
    "    alpha = np.zeros(k_regimes)\n",
    "    beta = np.zeros(k_regimes)\n",
    "    gamma = np.zeros(n_macro)\n",
    "    sigma2 = 1\n",
    "    P = np.full((k_regimes, k_regimes), 1/k_regimes)\n",
    "    s_t = np.random.choice(k_regimes, size=T)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Helper functions\n",
    "    # ---------------------------\n",
    "    def hamilton_filter(y, X_main, Z, alpha, beta, gamma, sigma2, P, pi0):\n",
    "        T = len(y)\n",
    "        k = len(alpha)\n",
    "        xi = np.zeros((T, k))\n",
    "        for s in range(k):\n",
    "            mu = alpha[s] + beta[s]*X_main[0] + np.dot(Z[0,:], gamma)\n",
    "            xi[0, s] = pi0[s] * norm.pdf(y[0], loc=mu, scale=np.sqrt(sigma2))\n",
    "        xi[0, :] /= xi[0, :].sum()\n",
    "        for t in range(1, T):\n",
    "            for s in range(k):\n",
    "                mu = alpha[s] + beta[s]*X_main[t] + np.dot(Z[t,:], gamma)\n",
    "                xi[t, s] = norm.pdf(y[t], loc=mu, scale=np.sqrt(sigma2)) * np.dot(xi[t-1, :], P[:, s])\n",
    "            xi[t, :] /= xi[t, :].sum()\n",
    "        return xi\n",
    "\n",
    "    def backward_sampling(xi, P):\n",
    "        T, k = xi.shape\n",
    "        s_t = np.zeros(T, dtype=int)\n",
    "        s_t[T-1] = np.random.choice(k, p=xi[T-1, :])\n",
    "        for t in range(T-2, -1, -1):\n",
    "            prob = xi[t, :] * P[:, s_t[t+1]]\n",
    "            prob /= prob.sum()\n",
    "            s_t[t] = np.random.choice(k, p=prob)\n",
    "        return s_t\n",
    "\n",
    "    # ---------------------------\n",
    "    # 3. Gibbs sampler loop\n",
    "    # ---------------------------\n",
    "    for it in range(n_iter):\n",
    "        # Step 1. Sample s_t\n",
    "        pi0 = np.full(k_regimes, 1/k_regimes)\n",
    "        xi = hamilton_filter(y, X_main, Z, alpha, beta, gamma, sigma2, P, pi0)\n",
    "        s_t = backward_sampling(xi, P)\n",
    "\n",
    "        # Step 2. Sample alpha_s and beta_s (regime-dependent)\n",
    "        for s in range(k_regimes):\n",
    "            idx = (s_t == s)\n",
    "            n_s = np.sum(idx)\n",
    "            if n_s > 0:\n",
    "                y_s = y[idx]\n",
    "                X_s = X_main[idx]\n",
    "                Z_s = Z[idx,:]\n",
    "                y_tilde = y_s - np.dot(Z_s, gamma)\n",
    "\n",
    "                # Sample beta_s\n",
    "                var_beta = 1 / (np.sum(X_s**2) / sigma2 + 1 / sigma2_beta[s])\n",
    "                mean_beta = var_beta * (np.sum(X_s * (y_tilde - alpha[s])) / sigma2)\n",
    "                beta[s] = norm.rvs(loc=mean_beta, scale=np.sqrt(var_beta))\n",
    "\n",
    "                # Sample alpha_s\n",
    "                var_alpha = 1 / (n_s / sigma2 + 1 / sigma2_alpha[s])\n",
    "                mean_alpha = var_alpha * (np.sum(y_tilde - beta[s] * X_s) / sigma2)\n",
    "                alpha[s] = norm.rvs(loc=mean_alpha, scale=np.sqrt(var_alpha))\n",
    "            else:\n",
    "                beta[s] = norm.rvs(loc=mu_beta[s], scale=np.sqrt(sigma2_beta[s]))\n",
    "                alpha[s] = norm.rvs(loc=mu_alpha[s], scale=np.sqrt(sigma2_alpha[s]))\n",
    "\n",
    "        # Enforce ordering constraint (Œ±_0 < Œ±_1)\n",
    "        sort_idx = np.argsort(alpha)\n",
    "        alpha = alpha[sort_idx]\n",
    "        beta = beta[sort_idx]\n",
    "        new_s_t = np.zeros_like(s_t)\n",
    "        for new_label, old_label in enumerate(sort_idx):\n",
    "            new_s_t[s_t == old_label] = new_label\n",
    "        s_t = new_s_t\n",
    "\n",
    "        # Step 3. Sample gamma (regime-invariant macro coefficients)\n",
    "        ZTZ = np.dot(Z.T, Z)\n",
    "        var_gamma = np.linalg.inv(ZTZ / sigma2 + np.diag(1 / sigma2_gamma))\n",
    "        y_resid = y - (alpha[s_t] + beta[s_t] * X_main)\n",
    "        mean_gamma = np.dot(var_gamma, np.dot(Z.T, y_resid) / sigma2)\n",
    "        gamma = np.random.multivariate_normal(mean_gamma, var_gamma)\n",
    "\n",
    "        # Step 4. Sample sigma2\n",
    "        resid = y - (alpha[s_t] + beta[s_t]*X_main + np.dot(Z, gamma))\n",
    "        alpha_post = alpha_sigma + T/2\n",
    "        beta_post = beta_sigma + 0.5 * np.sum(resid**2)\n",
    "        sigma2 = invgamma.rvs(a=alpha_post, scale=beta_post)\n",
    "\n",
    "        # Step 5. Sample transition matrix rows\n",
    "        counts = np.zeros((k_regimes, k_regimes))\n",
    "        for t in range(T-1):\n",
    "            counts[s_t[t], s_t[t+1]] += 1\n",
    "        for s in range(k_regimes):\n",
    "            P[s,:] = np.random.dirichlet(prior_trans[s,:] + counts[s,:])\n",
    "\n",
    "        # Store samples\n",
    "        alpha_samples[it,:] = alpha\n",
    "        beta_samples[it,:] = beta\n",
    "        gamma_samples[it,:] = gamma\n",
    "        sigma2_samples[it] = sigma2\n",
    "        s_samples[it,:] = s_t\n",
    "\n",
    "        if verbose and (it+1) % 100 == 0:\n",
    "            print(f\"Iteration {it+1} complete\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 4. Post-processing\n",
    "    # ---------------------------\n",
    "    results = {\n",
    "        'alpha_samples': alpha_samples[burn_in:],\n",
    "        'beta_samples': beta_samples[burn_in:],\n",
    "        'gamma_samples': gamma_samples[burn_in:],\n",
    "        'sigma2_samples': sigma2_samples[burn_in:],\n",
    "        's_samples': s_samples[burn_in:]\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Gibbs sampling with macro controls completed for model {model}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790dd5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_yt = run_multi_chain_gibbs(\n",
    "    df=df,\n",
    "    gibbs_function=estimate_cay_MS_gibbs_macro,\n",
    "    num_chains=4,\n",
    "    num_iterations=10000,\n",
    "    burn_in=2000,\n",
    "    k_regimes=2,\n",
    "    model='yt',\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd6a379",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_yt = run_multi_chain_gibbs(\n",
    "    df=df,\n",
    "    gibbs_function=estimate_cay_MS_gibbs_macro,\n",
    "    num_chains=4,\n",
    "    num_iterations=10000,\n",
    "    burn_in=2000,\n",
    "    k_regimes=2,\n",
    "    model='pca',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf45d660",
   "metadata": {},
   "source": [
    "‚úÖ Interpretation of Your PCA Model Traceplots and Density Plots\n",
    "Here is a structured evaluation for this new run:\n",
    "\n",
    "üî¨ 1. Parameter Œ±‚ÇÄ (alpha_0)\n",
    "Left (density): Chains overlap perfectly, unimodal Gaussian shape.\n",
    "\n",
    "Right (traceplot): Chains mix well, no drift, stable mean.\n",
    "\n",
    "‚û°Ô∏è Interpretation: Excellent convergence.\n",
    "\n",
    "üî¨ 2. Parameter Œ±‚ÇÅ (alpha_1)\n",
    "Left: Similar to Œ±‚ÇÄ, smooth single mode, tight overlap.\n",
    "\n",
    "Right: Stable traceplots, good mixing.\n",
    "\n",
    "‚û°Ô∏è Interpretation: Excellent convergence.\n",
    "\n",
    "üî¨ 3. Parameter Œ≤‚ÇÄ (beta_0)\n",
    "Left: Overlapping Gaussian densities across chains.\n",
    "\n",
    "Right: Traceplots stationary with full mixing.\n",
    "\n",
    "‚û°Ô∏è Interpretation: Excellent convergence.\n",
    "\n",
    "üî¨ 4. Parameter Œ≤‚ÇÅ (beta_1)\n",
    "Left: Perfect overlap, unimodal.\n",
    "\n",
    "Right: Stable traceplots with high frequency variation indicating good mixing.\n",
    "\n",
    "‚û°Ô∏è Interpretation: Excellent convergence.\n",
    "\n",
    "üî¨ 5. Parameter œÉ¬≤ (sigma2)\n",
    "Left: Gaussian shape, all chains identical.\n",
    "\n",
    "Right: Traceplots stable, good mixing.\n",
    "\n",
    "‚û°Ô∏è Interpretation: Excellent convergence.\n",
    "\n",
    "‚úÖ Overall PCA Model Convergence Diagnosis\n",
    "‚úÖ All parameters show full convergence\n",
    "‚úÖ No label switching or multimodality issues detected\n",
    "‚úÖ Chains mix well and are stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c6e470",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs_yt['df'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea83800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_pca = estimate_cay_MS_via_gibbs_final(df, model='pca', n_iter=10000, burn_in=2000, verbose=True)\n",
    "outputs_pca_macro = estimate_cay_MS_gibbs_macro(df, k_regimes=2, n_iter=1000, burn_in=200, model='pca', verbose=True)\n",
    "outputs_yt = estimate_cay_MS_via_gibbs_final(df, model='yt', n_iter=10000, burn_in=2000, verbose=True)\n",
    "outputs_yt_macro = estimate_cay_MS_gibbs_macro(df, k_regimes=2, n_iter=1000, burn_in=200, model='yt', verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44ab3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_cay_FC_vs_MS(outputs_pca['df'], horizons, model='pca')\n",
    "compare_cay_FC_vs_MS(outputs_yt['df'], horizons, model='yt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997cd27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_cay_FC_vs_MS_with_macro(outputs_yt['df'], horizons, model='yt')\n",
    "compare_cay_FC_vs_MS_with_macro(outputs_pca['df'], horizons, model='pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555fe006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# === Extract MLE smoothed probabilities ===\n",
    "\n",
    "# Assuming 2 regimes\n",
    "mle_probs_pca = ms_result_cay_pca.smoothed_marginal_probabilities\n",
    "\n",
    "mle_regime0_pca = mle_probs_pca[0].values\n",
    "mle_regime1_pca = mle_probs_pca[1].values\n",
    "\n",
    "# === Compute Bayesian posterior regime probabilities ===\n",
    "\n",
    "# s_samples_pca: (iterations, T)\n",
    "bayesian_regime0_pca = (outputs_pca['s_samples'] == 0).mean(axis=0)\n",
    "bayesian_regime1_pca = (outputs_pca['s_samples'] == 1).mean(axis=0)\n",
    "\n",
    "# === Plot comparison ===\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Regime 0\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(mle_regime0_pca, label='MLE Regime 0', alpha=0.7)\n",
    "plt.plot(bayesian_regime0_pca, label='Bayesian Regime 0', alpha=0.7)\n",
    "plt.title('PCA Regime 0 Probability Comparison')\n",
    "plt.legend()\n",
    "\n",
    "# Regime 1\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(mle_regime1_pca, label='MLE Regime 1', alpha=0.7)\n",
    "plt.plot(bayesian_regime1_pca, label='Bayesian Regime 1', alpha=0.7)\n",
    "plt.title('PCA Regime 1 Probability Comparison')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042c0fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# === Extract MLE smoothed probabilities ===\n",
    "\n",
    "# Assuming 2 regimes\n",
    "mle_probs_yt = ms_result_cay_yt.smoothed_marginal_probabilities\n",
    "\n",
    "mle_regime0_yt = mle_probs_yt[0].values\n",
    "mle_regime1_yt = mle_probs_yt[1].values\n",
    "\n",
    "# === Compute Bayesian posterior regime probabilities ===\n",
    "\n",
    "# s_samples_pca: (iterations, T)\n",
    "bayesian_regime0_yt = (outputs_yt['s_samples'] == 0).mean(axis=0)\n",
    "bayesian_regime1_yt = (outputs_yt['s_samples'] == 1).mean(axis=0)\n",
    "\n",
    "# === Plot comparison ===\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Regime 0\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(mle_regime0_yt, label='MLE Regime 0', alpha=0.7)\n",
    "plt.plot(bayesian_regime0_yt, label='Bayesian Regime 0', alpha=0.7)\n",
    "plt.title('PCA Regime 0 Probability Comparison')\n",
    "plt.legend()\n",
    "\n",
    "# Regime 1\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(mle_regime1_yt, label='MLE Regime 1', alpha=0.7)\n",
    "plt.plot(bayesian_regime1_yt, label='Bayesian Regime 1', alpha=0.7)\n",
    "plt.title('PCA Regime 1 Probability Comparison')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7d0d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# === Extract MLE smoothed probabilities ===\n",
    "\n",
    "# Assuming 2 regimes\n",
    "mle_probs_pca_with_macro = ms_result_cay_pca_with_macro.smoothed_marginal_probabilities\n",
    "\n",
    "mle_regime0_pca = mle_probs_pca_with_macro[0].values\n",
    "mle_regime1_pca = mle_probs_pca_with_macro[1].values\n",
    "\n",
    "# === Compute Bayesian posterior regime probabilities ===\n",
    "\n",
    "# s_samples_pca: (iterations, T)\n",
    "bayesian_regime0_pca = (outputs_pca_macro['s_samples'] == 0).mean(axis=0)\n",
    "bayesian_regime1_pca = (outputs_pca_macro['s_samples'] == 1).mean(axis=0)\n",
    "\n",
    "# === Plot comparison ===\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Regime 0\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(mle_regime0_pca, label='MLE Regime 0', alpha=0.7)\n",
    "plt.plot(bayesian_regime0_pca, label='Bayesian Regime 0', alpha=0.7)\n",
    "plt.title('PCA Regime 0 Probability Comparison')\n",
    "plt.legend()\n",
    "\n",
    "# Regime 1\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(mle_regime1_pca, label='MLE Regime 1', alpha=0.7)\n",
    "plt.plot(bayesian_regime1_pca, label='Bayesian Regime 1', alpha=0.7)\n",
    "plt.title('PCA Regime 1 Probability Comparison')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5688640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# === Extract MLE smoothed probabilities ===\n",
    "\n",
    "# Assuming 2 regimes\n",
    "mle_probs_yt_with_macro = ms_result_cay_yt_with_macro.smoothed_marginal_probabilities\n",
    "\n",
    "mle_regime0_yt_with_macro = mle_probs_yt_with_macro[0].values\n",
    "mle_regime1_yt_with_macro = mle_probs_yt_with_macro[1].values\n",
    "\n",
    "# === Compute Bayesian posterior regime probabilities ===\n",
    "\n",
    "# s_samples_pca: (iterations, T)\n",
    "bayesian_regime0_yt_with_macro = (outputs_yt_macro['s_samples'] == 0).mean(axis=0)\n",
    "bayesian_regime1_yt_with_macro = (outputs_yt_macro['s_samples'] == 1).mean(axis=0)\n",
    "\n",
    "# === Plot comparison ===\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Regime 0\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(mle_regime0_yt_with_macro, label='MLE Regime 0', alpha=0.7)\n",
    "plt.plot(bayesian_regime0_yt_with_macro, label='Bayesian Regime 0', alpha=0.7)\n",
    "plt.title('Yt Regime 0 Probability Comparison')\n",
    "plt.legend()\n",
    "\n",
    "# Regime 1\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(mle_regime1_yt_with_macro, label='MLE Regime 1', alpha=0.7)\n",
    "plt.plot(bayesian_regime1_yt_with_macro, label='Bayesian Regime 1', alpha=0.7)\n",
    "plt.title('Yt Regime 1 Probability Comparison')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868c6fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# === Extract MLE smoothed probabilities ===\n",
    "\n",
    "# Assuming 2 regimes\n",
    "mle_probs_yt = ms_result_cay_yt.smoothed_marginal_probabilities\n",
    "\n",
    "mle_regime0_yt = mle_probs_yt[0].values\n",
    "mle_regime1_yt = mle_probs_yt[1].values\n",
    "\n",
    "# === Compute Bayesian posterior regime probabilities ===\n",
    "\n",
    "# s_samples_pca: (iterations, T)\n",
    "bayesian_regime0_yt = (outputs_yt['s_samples'] == 0).mean(axis=0)\n",
    "bayesian_regime1_yt = (outputs_yt['s_samples'] == 1).mean(axis=0)\n",
    "\n",
    "# === Plot comparison ===\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Regime 0\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(mle_regime0_yt, label='MLE Regime 0', alpha=0.7)\n",
    "plt.plot(bayesian_regime0_yt, label='Bayesian Regime 0', alpha=0.7)\n",
    "plt.title('Yt Regime 0 Probability Comparison')\n",
    "plt.legend()\n",
    "\n",
    "# Regime 1\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(mle_regime1_yt, label='MLE Regime 1', alpha=0.7)\n",
    "plt.plot(bayesian_regime1_yt, label='Bayesian Regime 1', alpha=0.7)\n",
    "plt.title('Yt Regime 1 Probability Comparison')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ae78af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns\n",
    "macro_controls = ['interest_rate', 'CPI_inflation', 'unemployment']\n",
    "\n",
    "# Drop rows with missing values to ensure consistent regression dataset\n",
    "df_macro = df[['future_ret_1q', 'cay_FC_yt'] + macro_controls].dropna()\n",
    "\n",
    "# Extract target and predictors\n",
    "Y = df_macro['future_ret_1q']\n",
    "X = df_macro[['cay_FC_yt'] + macro_controls]\n",
    "\n",
    "# Add constant for intercept\n",
    "import statsmodels.api as sm\n",
    "X = sm.add_constant(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e63cbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit OLS regression\n",
    "model = sm.OLS(Y, X)\n",
    "results = model.fit()\n",
    "\n",
    "# Print summary\n",
    "print(results.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cacc1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs_yt['df'].info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75362b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_ms prepared as above\n",
    "Y_ms = df['future_ret_1q']\n",
    "X_ms = outputs_yt['df'][['cay_MS_yt'] + macro_controls].dropna()\n",
    "X_ms = sm.add_constant(X_ms)\n",
    "\n",
    "model_ms = sm.OLS(Y_ms, X_ms)\n",
    "results_ms = model_ms.fit()\n",
    "\n",
    "print(results_ms.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040eca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_cay_FC_vs_MS_with_macro(outputs_yt['df'], horizons, model='yt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3d0c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Calculate VIF for each variable\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"variable\"] = X_ms.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n",
    "                   for i in range(X.shape[1])]\n",
    "\n",
    "print(vif_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a037e52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Calculate VIF for each variable\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"variable\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n",
    "                   for i in range(X.shape[1])]\n",
    "print(vif_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e4fccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute correlation matrix for df_macro\n",
    "corr = df_macro.corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\", square=True)\n",
    "plt.title('Correlation Matrix (df_macro)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67820c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_cay_FC_vs_MS(df, horizons, model='yt')\n",
    "compare_cay_FC_vs_MS(df, horizons, model='yt_at')\n",
    "compare_cay_FC_vs_MS(df, horizons, model='pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a9f9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_all_cay_FC_vs_MS_results\n",
    "\n",
    "all_results = load_all_cay_FC_vs_MS_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6902d760",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate posterior regime probabilities\n",
    "regime_probs = np.mean(outputs_yt['s_samples'], axis=0)\n",
    "\n",
    "# Slice regime_probs to match df\n",
    "regime_probs_matched = regime_probs[-len(df.index):]\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(df.index, regime_probs_matched, label='Regime 1 Posterior Probability with yt_at')\n",
    "plt.title('Estimated Regime 1 Posterior Probability over Time')\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1e5673",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate posterior regime probabilities\n",
    "regime_probs = np.mean(outputs_pca['s_samples'], axis=0)\n",
    "\n",
    "# Plot\n",
    "# Slice regime_probs to match df\n",
    "regime_probs_matched = regime_probs[-len(df.index):]\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(df.index, regime_probs_matched, label='Regime 1 Posterior Probability with yt')\n",
    "plt.title('Estimated Regime 1 Posterior Probability over Time')\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85a41a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate posterior regime probabilities\n",
    "regime_probs = np.mean(s_samples_post_yt_at, axis=0)\n",
    "\n",
    "# Slice regime_probs to match df\n",
    "regime_probs_matched = regime_probs[-len(df.index):]\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(df.index, regime_probs_matched, label='Regime 1 Posterior Probability with yt_at')\n",
    "plt.title('Estimated Regime 1 Posterior Probability over Time')\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cefb859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming smoothed_probs is your regime probability DataFrame\n",
    "# regime_probs_df = smoothed_probs[[1]].rename(columns={1: 'regime1_prob'})\n",
    "# df = df.merge(regime_probs_df, left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84e1231",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reg[[cay_var, 'interest_rate', 'CPI_inflation']]\n",
    "X = sm.add_constant(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ccbd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm, invgamma\n",
    "\n",
    "def estimate_cay_MS_via_gibbs_version_3(df, k_regimes=2, n_iter=1000, burn_in=200, model='yt', verbose=True):\n",
    "    \"\"\"\n",
    "    Estimate cay_MS using a Markov-switching regression with Gibbs sampling.\n",
    "    Implements Bianchi et al. (2016) recommendations:\n",
    "    - Calibrated Dirichlet priors\n",
    "    - Ordering constraint (Œ±_0 < Œ±_1)\n",
    "    Returns: dict with df and posterior samples\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------------------------\n",
    "    # 0. Data preparation\n",
    "    # ---------------------------\n",
    "    if model == 'yt':\n",
    "        y = df['log_ct'].values\n",
    "        X = df['log_yt'].values\n",
    "    elif model == 'pca':\n",
    "        y = df['log_ct'].values\n",
    "        X = df['pca'].values\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model}\")\n",
    "\n",
    "    T = len(y)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 1. Hyperparameters\n",
    "    # ---------------------------\n",
    "    sigma2_y = np.var(y)\n",
    "    mu_alpha = np.zeros(k_regimes)\n",
    "    sigma2_alpha = np.ones(k_regimes) * sigma2_y\n",
    "    mu_beta = 0\n",
    "    sigma2_beta = sigma2_y\n",
    "    alpha_sigma = 2.5\n",
    "    beta_sigma = 0.5\n",
    "    prior_trans = np.ones((k_regimes, k_regimes)) * 2  # Beta(2,2)-equivalent\n",
    "\n",
    "    # ---------------------------\n",
    "    # 2. Initialize storage\n",
    "    # ---------------------------\n",
    "    alpha_samples = np.zeros((n_iter, k_regimes))\n",
    "    beta_samples = np.zeros(n_iter)\n",
    "    sigma2_samples = np.zeros(n_iter)\n",
    "    s_samples = np.zeros((n_iter, T))\n",
    "\n",
    "    # Initialize parameters\n",
    "    alpha = np.zeros(k_regimes)\n",
    "    beta = 0\n",
    "    sigma2 = 1\n",
    "    P = np.full((k_regimes, k_regimes), 1/k_regimes)\n",
    "    s_t = np.random.choice(k_regimes, size=T)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Helper functions\n",
    "    # ---------------------------\n",
    "    def hamilton_filter(y, X, alpha, beta, sigma2, P, pi0):\n",
    "        T = len(y)\n",
    "        k = len(alpha)\n",
    "        xi = np.zeros((T, k))\n",
    "        for s in range(k):\n",
    "            mu = alpha[s] + beta * X[0]\n",
    "            xi[0, s] = pi0[s] * norm.pdf(y[0], loc=mu, scale=np.sqrt(sigma2))\n",
    "        xi[0, :] /= xi[0, :].sum()\n",
    "        for t in range(1, T):\n",
    "            for s in range(k):\n",
    "                mu = alpha[s] + beta * X[t]\n",
    "                xi[t, s] = norm.pdf(y[t], loc=mu, scale=np.sqrt(sigma2)) * np.dot(xi[t-1, :], P[:, s])\n",
    "            xi[t, :] /= xi[t, :].sum()\n",
    "        return xi\n",
    "\n",
    "    def backward_sampling(xi, P):\n",
    "        T, k = xi.shape\n",
    "        s_t = np.zeros(T, dtype=int)\n",
    "        s_t[T-1] = np.random.choice(k, p=xi[T-1, :])\n",
    "        for t in range(T-2, -1, -1):\n",
    "            prob = xi[t, :] * P[:, s_t[t+1]]\n",
    "            prob /= prob.sum()\n",
    "            s_t[t] = np.random.choice(k, p=prob)\n",
    "        return s_t\n",
    "\n",
    "    # ---------------------------\n",
    "    # 5. Gibbs sampler loop\n",
    "    # ---------------------------\n",
    "    for it in range(n_iter):\n",
    "        # Step 1. Sample s_t using Hamilton filter + backward sampling\n",
    "        pi0 = np.full(k_regimes, 1/k_regimes)\n",
    "        xi = hamilton_filter(y, X, alpha, beta, sigma2, P, pi0)\n",
    "        s_t = backward_sampling(xi, P)\n",
    "\n",
    "        # Step 2. Sample alpha_s with ordering constraint\n",
    "        for s in range(k_regimes):\n",
    "            idx = (s_t == s)\n",
    "            n_s = np.sum(idx)\n",
    "            if n_s > 0:\n",
    "                y_s = y[idx]\n",
    "                X_s = X[idx]\n",
    "                var_alpha = 1 / (n_s / sigma2 + 1 / sigma2_alpha[s])\n",
    "                var_alpha = max(var_alpha, 1e-8)\n",
    "                mean_alpha = var_alpha * (np.sum(y_s - beta * X_s) / sigma2)\n",
    "                alpha[s] = norm.rvs(loc=mean_alpha, scale=np.sqrt(var_alpha))\n",
    "            else:\n",
    "                alpha[s] = norm.rvs(loc=mu_alpha[s], scale=np.sqrt(sigma2_alpha[s]))\n",
    "\n",
    "        # Enforce ordering constraint (Œ±_0 < Œ±_1)\n",
    "        sort_idx = np.argsort(alpha)\n",
    "        alpha = alpha[sort_idx]\n",
    "        new_s_t = np.zeros_like(s_t)\n",
    "        for new_label, old_label in enumerate(sort_idx):\n",
    "            new_s_t[s_t == old_label] = new_label\n",
    "        s_t = new_s_t\n",
    "\n",
    "        # Step 3. Sample beta\n",
    "        y_tilde = y - alpha[s_t]\n",
    "        var_beta = 1 / (np.sum(X**2) / sigma2 + 1 / sigma2_beta)\n",
    "        var_beta = max(var_beta, 1e-8)\n",
    "        mean_beta = var_beta * (np.sum(X * y_tilde) / sigma2)\n",
    "        beta = norm.rvs(loc=mean_beta, scale=np.sqrt(var_beta))\n",
    "\n",
    "        # Step 4. Sample sigma2\n",
    "        resid = y - alpha[s_t] - beta * X\n",
    "        alpha_post = alpha_sigma + T/2\n",
    "        beta_post = beta_sigma + 0.5 * np.sum(resid**2)\n",
    "        sigma2 = invgamma.rvs(a=alpha_post, scale=beta_post)\n",
    "\n",
    "        # Step 5. Sample transition matrix rows\n",
    "        counts = np.zeros((k_regimes, k_regimes))\n",
    "        for t in range(T-1):\n",
    "            counts[s_t[t], s_t[t+1]] += 1\n",
    "        for s in range(k_regimes):\n",
    "            P[s,:] = np.random.dirichlet(prior_trans[s,:] + counts[s,:])\n",
    "\n",
    "        # Store samples\n",
    "        alpha_samples[it,:] = alpha\n",
    "        beta_samples[it] = beta\n",
    "        sigma2_samples[it] = sigma2\n",
    "        s_samples[it,:] = s_t\n",
    "\n",
    "        if verbose and (it+1) % 100 == 0:\n",
    "            print(f\"Iteration {it+1} complete\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 6. Post-processing\n",
    "    # ---------------------------\n",
    "    alpha_samples_ = alpha_samples[burn_in:]\n",
    "    beta_samples_ = beta_samples[burn_in:]\n",
    "    sigma2_samples_ = sigma2_samples[burn_in:]\n",
    "    s_samples_ = s_samples[burn_in:]\n",
    "\n",
    "    alpha_mean = alpha_samples_.mean(axis=0)\n",
    "    beta_mean = beta_samples_.mean()\n",
    "    s_t_mode = np.round(s_samples_.mean(axis=0)).astype(int)\n",
    "    cay_MS = y - (alpha_mean[s_t_mode] + beta_mean * X)\n",
    "\n",
    "    # Add to dataframe\n",
    "    df = df.copy()\n",
    "    df[f'cay_MS_{model}'] = cay_MS\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Full Gibbs sampling with Hamilton filter completed. cay_MS for model {model} calculated and added to dataframe.\")\n",
    "\n",
    "    # Return results dict\n",
    "    return {\n",
    "        'df': df,\n",
    "        'alpha_samples': alpha_samples_,\n",
    "        'beta_samples': beta_samples_,\n",
    "        'sigma2_samples': sigma2_samples_,\n",
    "        's_samples': s_samples_\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010531d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def summarize_gibbs_results(df, s_samples_post, alpha_samples, beta_samples, sigma2_samples, model_label='yt'):\n",
    "    \"\"\"\n",
    "    Summarize Gibbs sampler outputs:\n",
    "    - Plots posterior regime probabilities\n",
    "    - Returns parameter summary dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # ---------------------------\n",
    "    # 1. Calculate posterior regime probabilities\n",
    "    # ---------------------------\n",
    "    regime_probs = np.mean(s_samples_post, axis=0)\n",
    "\n",
    "    # Align regime_probs with df index\n",
    "    if len(regime_probs) > len(df.index):\n",
    "        regime_probs_matched = regime_probs[-len(df.index):]\n",
    "    elif len(regime_probs) < len(df.index):\n",
    "        raise ValueError(\"regime_probs length is shorter than df index. Check input alignment.\")\n",
    "    else:\n",
    "        regime_probs_matched = regime_probs\n",
    "\n",
    "    # Plot regime probabilities\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(df.index, regime_probs_matched, label=f'Regime 1 Posterior Probability with {model_label}')\n",
    "    plt.title(f'Estimated Regime 1 Posterior Probability over Time ({model_label})')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.xlabel('Date')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Save to results dict\n",
    "    results['regime_probs'] = regime_probs_matched\n",
    "\n",
    "    # ---------------------------\n",
    "    # 2. Parameter posterior summaries\n",
    "    # ---------------------------\n",
    "    alpha_mean = alpha_samples.mean(axis=0)\n",
    "    alpha_std = alpha_samples.std(axis=0)\n",
    "    beta_mean = beta_samples.mean()\n",
    "    beta_std = beta_samples.std()\n",
    "    sigma2_mean = sigma2_samples.mean()\n",
    "    sigma2_std = sigma2_samples.std()\n",
    "\n",
    "    # Combine into a dataframe\n",
    "    param_summary = pd.DataFrame({\n",
    "        'Parameter': [f'alpha_{i}' for i in range(len(alpha_mean))] + ['beta', 'sigma2'],\n",
    "        'Mean': np.concatenate([alpha_mean, [beta_mean, sigma2_mean]]),\n",
    "        'Std': np.concatenate([alpha_std, [beta_std, sigma2_std]])\n",
    "    })\n",
    "\n",
    "    print(\"\\nüî∑ Posterior Parameter Summary:\")\n",
    "    print(param_summary)\n",
    "\n",
    "    # Save to results dict\n",
    "    results['param_summary'] = param_summary\n",
    "\n",
    "    # ---------------------------\n",
    "    # 3. Effective sample size diagnostics (optional)\n",
    "    # ---------------------------\n",
    "    # Example: simple ESS estimate (inefficient but quick)\n",
    "    def ess(x):\n",
    "        n = len(x)\n",
    "        acf_sum = 0\n",
    "        for lag in range(1, min(1000, n)):\n",
    "            acf = np.corrcoef(x[:-lag], x[lag:])[0,1]\n",
    "            if np.isnan(acf) or acf < 0:\n",
    "                break\n",
    "            acf_sum += 2 * acf\n",
    "        return n / (1 + acf_sum)\n",
    "\n",
    "    beta_ess = ess(beta_samples)\n",
    "    sigma2_ess = ess(sigma2_samples)\n",
    "\n",
    "    print(f\"\\nüî∑ Effective Sample Size (ESS): beta = {beta_ess:.1f}, sigma2 = {sigma2_ess:.1f}\")\n",
    "\n",
    "    results['ess_beta'] = beta_ess\n",
    "    results['ess_sigma2'] = sigma2_ess\n",
    "\n",
    "    # Return results dictionary\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e063cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = estimate_cay_MS_via_gibbs_version_3(df, model='yt', verbose=True)\n",
    "\n",
    "summary = summarize_gibbs_results(\n",
    "    df = outputs['df'],\n",
    "    s_samples_post = outputs['s_samples'],\n",
    "    alpha_samples = outputs['alpha_samples'],\n",
    "    beta_samples = outputs['beta_samples'],\n",
    "    sigma2_samples = outputs['sigma2_samples'],\n",
    "    model_label = 'yt'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fc974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = estimate_cay_MS_via_gibbs_version_3(df, model='pca', verbose=True)\n",
    "\n",
    "summary = summarize_gibbs_results(\n",
    "    df = outputs['df'],\n",
    "    s_samples_post = outputs['s_samples'],\n",
    "    alpha_samples = outputs['alpha_samples'],\n",
    "    beta_samples = outputs['beta_samples'],\n",
    "    sigma2_samples = outputs['sigma2_samples'],\n",
    "    model_label = 'pca'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3743725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_implied_sharpe(y_true, y_pred, freq=4):\n",
    "    \"\"\"\n",
    "    Computes the implied Sharpe ratio from regression forecast residuals.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: array-like, true realized returns\n",
    "    - y_pred: array-like, predicted returns from regression\n",
    "    - freq: int, annualization factor (e.g. 4 for quarterly data)\n",
    "\n",
    "    Returns:\n",
    "    - sr_annualized: annualized Sharpe ratio\n",
    "    \"\"\"\n",
    "    # Compute residuals\n",
    "    residuals = y_true - y_pred\n",
    "\n",
    "    # Standard deviation of true returns and residuals\n",
    "    sigma_r = np.std(y_true, ddof=1)\n",
    "    sigma_e = np.std(residuals, ddof=1)\n",
    "\n",
    "    # R-squared\n",
    "    r_squared = 1 - (np.var(residuals, ddof=1) / np.var(y_true, ddof=1))\n",
    "\n",
    "    # Implied Sharpe ratio (unannualized)\n",
    "    sr = np.sqrt(max(r_squared,0))\n",
    "\n",
    "    # Annualize\n",
    "    sr_annualized = sr * np.sqrt(freq)\n",
    "\n",
    "    print(f\"R¬≤: {r_squared:.4f}\")\n",
    "    print(f\"Implied unannualized SR: {sr:.4f}\")\n",
    "    print(f\"Implied annualized SR: {sr_annualized:.4f}\")\n",
    "\n",
    "    return sr_annualized\n",
    "\n",
    "# === Example usage ===\n",
    "\n",
    "# Replace with your actual regression inputs\n",
    "# y_true = df['future_ret_4q'].values\n",
    "# y_pred = regression_model.predict(df[['const', 'cay_FC_yt']])\n",
    "\n",
    "# sr = compute_implied_sharpe(y_true, y_pred, freq=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
